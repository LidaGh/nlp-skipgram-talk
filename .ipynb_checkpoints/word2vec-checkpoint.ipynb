{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "\n",
    "class Word2Vec:\n",
    "    def get_variable(self, shape, name, init='normal'):\n",
    "        if init == 'uniform':\n",
    "            initializer = tf.random_uniform(shape=shape, minval=-1, maxval=1)\n",
    "        else:\n",
    "            initializer = tf.truncated_normal(shape=shape, stddev=STDDEV)\n",
    "        return tf.get_variable(name, initializer=initializer)\n",
    "    \n",
    "    def fc(self, x, dim_in, dim_out, name):\n",
    "        W = self.get_variable(name=name+\"_W\", shape=[dim_in, dim_out])\n",
    "        b = self.get_variable(name=name+\"_b\", shape=[dim_out])\n",
    "        return tf.matmul(x, W) + b, W, b\n",
    "\n",
    "    def train(self, session, train_x, train_y):\n",
    "        total_loss = 0.0\n",
    "        total_entries = 0\n",
    "        batches_count = len(train_x)\n",
    "        for i in trange(batches_count):\n",
    "            batch_x, batch_y = train_x[i], train_y[i]\n",
    "            _, loss = session.run([self.train_op, self.loss])\n",
    "#             print('loss', loss)\n",
    "            total_loss += np.sum(loss)\n",
    "            total_entries += len(loss)\n",
    "        if total_entries == 0:\n",
    "            return 0\n",
    "        return total_loss / total_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla SkipGram\n",
    "\n",
    "- Input size equal to size of the vocabulary\n",
    "- Single hidden layer\n",
    "- Output layer with softmax of size of the vocabulary\n",
    "- Loss function - cross entropy\n",
    "- Gradient Descent\n",
    "- Embeding - first layer - `W + b` (with broadcasting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(Word2Vec):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        self.input  = tf.placeholder(tf.float32, shape=[None, vocab_size], name=\"input\")\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, vocab_size], name=\"labels\")\n",
    "\n",
    "        # Build 2-layers FFNN\n",
    "        self.hidden_layer, W, b = self.fc(self.input,        vocab_size, embedding_size, name=\"layer_1\")\n",
    "        self.logits, _, _       = self.fc(self.hidden_layer, embedding_size, vocab_size, name=\"layer_2\")\n",
    "\n",
    "        # Store first layer\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        # Define training tensors\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(self.loss)\n",
    "    \n",
    "    def get_embedding(self, session):\n",
    "        return session.run(self.W + self.b)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram with NegativeSampling\n",
    "\n",
    "- Input accepts values of `word2int` - dimension `[BATCH_SIZE]`\n",
    "- One hidden layer, but used as embedding lookup (no bias)\n",
    "- Output layer with sampled softmax; Computes softmax based on one positive and several negative samples\n",
    "- Adam\n",
    "- Embedding - first layer - `W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegativeSampling(Word2Vec):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        self.input, self.labels = iterator.get_next()\n",
    "#         self.input  = tf.placeholder(tf.int64, shape=[None], name=\"input\")\n",
    "#         self.labels = tf.placeholder(tf.int64, shape=[None, None], name=\"labels\")\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = self.get_variable(name=\"layer_1_W\", shape=[vocab_size, embedding_size], init=\"uniform\")\n",
    "        inputs = tf.nn.embedding_lookup(self.embedding, self.input)\n",
    "\n",
    "        W = self.get_variable(name=\"layer_2_W\", shape=[vocab_size, embedding_size])\n",
    "        b = self.get_variable(name=\"layer_2_b\", shape=[vocab_size])\n",
    "\n",
    "        self.negative_sampling(W, b, inputs)\n",
    "        \n",
    "    def negative_sampling(self, W, b, inputs):\n",
    "        self.loss = tf.nn.sampled_softmax_loss(\n",
    "            weights=self.embedding, \n",
    "            biases=b, \n",
    "            labels=self.labels, \n",
    "            inputs=inputs,\n",
    "            num_sampled=NUM_SAMPLED,\n",
    "            num_classes=self.vocab_size)\n",
    "#         self.loss = tf.nn.nce_loss(\n",
    "#             weights=W,\n",
    "#             biases=b,\n",
    "#             labels=self.labels, \n",
    "#             inputs=inputs,\n",
    "#             num_sampled=NUM_SAMPLED,\n",
    "#             num_classes=self.vocab_size\n",
    "#         )\n",
    "        cost = tf.reduce_mean(self.loss)\n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(cost)\n",
    "        # self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        # self.train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(self.loss)\n",
    "\n",
    "    def get_embedding(self, session):\n",
    "        return session.run(self.embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmannNode:\n",
    "    def __init__(self, dimension):\n",
    "        self.representation = tf.Variable(tf.random_uniform(shape=[dimension], minval=-1, maxval=1))\n",
    "    \n",
    "    def set_left(self, node):\n",
    "        self.left = node\n",
    "        \n",
    "    def set_right(self, node):\n",
    "        self.right = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramHierarchicalSoftmax(Word2Vec):\n",
    "    def __init__(self, vocab_size, embedding_size, word_infrequency, word2int):\n",
    "        self.input  = tf.placeholder(tf.int64, shape=[None], name=\"input\")\n",
    "        self.labels = tf.placeholder(tf.int64, shape=[None, 1], name=\"labels\")\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.word2int = word2int\n",
    "        \n",
    "        self.probabilities = [None] * vocab_size\n",
    "        \n",
    "        self.embedding = self.get_variable(name=\"layer_1_W\", shape=[vocab_size, embedding_size], init=\"uniform\")\n",
    "        x = tf.nn.embedding_lookup(self.embedding, self.input)\n",
    "\n",
    "        self.root = self.build_huffmann_tree(word_infrequency)\n",
    "        self.i = 0\n",
    "        self.traverse_huffman_tree(self.root, x, 1)\n",
    "        \n",
    "        self.final_probabilities = tf.concat([[x] for x in self.probabilities], axis=0)\n",
    "        \n",
    "        self.labels_probabilities = tf.gather(self.final_probabilities, self.labels)\n",
    "        \n",
    "        self.loss = -tf.log(self.labels_probabilities + 1e-10)\n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "    def build_huffmann_tree(self, word_infrequency):\n",
    "        # Huffmann\n",
    "        while len(word_infrequency) > 1:\n",
    "            x, y = word_infrequency.most_common(2) # 2 least frequent words\n",
    "            \n",
    "            node = HuffmannNode(dimension=self.embedding_size)\n",
    "            node.set_left(x[0])\n",
    "            node.set_right(y[0])\n",
    "            \n",
    "            word_infrequency.pop(x[0])\n",
    "            word_infrequency.pop(y[0])\n",
    "            word_infrequency[node] = x[1] + y[1]\n",
    "        \n",
    "        root = word_infrequency.most_common(1)[0][0]\n",
    "        return root\n",
    "    \n",
    "    def traverse_huffman_tree(self, node, v_wi, probability):\n",
    "        if type(node) is str:\n",
    "            self.i += 1\n",
    "            self.probabilities[self.word2int[node]] = probability\n",
    "            if self.i % 100 == 0:\n",
    "                print(self.i, \"/\", self.vocab_size)\n",
    "            return None\n",
    "        \n",
    "        self.traverse_huffman_tree(node.left, v_wi, \n",
    "                                   probability * tf.sigmoid(-tf.reduce_sum(v_wi * node.representation)))\n",
    "        self.traverse_huffman_tree(node.right, v_wi,\n",
    "                                   probability * tf.sigmoid(tf.reduce_sum(v_wi * node.representation)))\n",
    "        \n",
    "\n",
    "    def get_embedding(self, session):\n",
    "        return session.run(self.embedding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a basic text corpus using Keras i.e. the IMDB one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out what one of the reviews actually says!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this is a bit long 2 hours 20 minutes but it had a a lot of the famous ? ? novel in it in other words a lot of ? to ? br br it was ? ? at times but had some ? dramatic moments too ? off by a ? ? at the end of the film that was ? to view ? this film is about ? years old the special effects ? on this film did a ? job br br paul ? and ? ? were ? ? actors in their day and they don't ? here both giving powerful performances the only problem is ? as all the ? are played by ? and some of them like ? ? just don't look real i'd like to see a re make of this movie with all ? actors not for ? ? but to simply make the story look and sound more ?\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "reverse_word_index = list( word_index.items() ) \\\n",
    "    | select( lambda i: (i[1],i[0]) ) \\\n",
    "    | as_dict\n",
    "\n",
    "decoded_review = train_data[review] \\\n",
    "    | select( lambda i: reverse_word_index.get(i - 3, '?') ) \\\n",
    "    | as_list\n",
    "\n",
    "print( ' '.join(decoded_review) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "sentence = to_categorical(train_data[review], num_classes=1000)\n",
    "\n",
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the reviews together for a contrived corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = range(25000) \\\n",
    "    | select( lambda i: train_data[0] ) \\\n",
    "    | chain \\\n",
    "    | as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "vocab_size = 1000\n",
    "window_size = 3\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(all_reviews, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3648132,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-21e3f3fc593e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdot_axes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"glorot_uniform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Merge' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "vocab_size = 1000\n",
    "embed_size = 300\n",
    "\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))\n",
    "word_model.add(Reshape((embed_size,)))\n",
    "\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embed_size,\n",
    "                            embeddings_initializer=\"glorot_uniform\",\n",
    "                            input_length=1))\n",
    "context_model.add(Reshape((embed_size,)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([word_model, context_model], mode=\"dot\", dot_axes=0))\n",
    "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "merge_layer = model.layers[0]\n",
    "word_model = merge_layer.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 300)       300000      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300, 1)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 300, 1)       0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 1)         0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            2           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 300,002\n",
      "Trainable params: 300,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "svg_code = model_to_dot(validation_model).create(prog='dot', format='svg')\n",
    "\n",
    "with open(\"validation_model.svg\", \"wb\") as text_file:\n",
    "    text_file.write(svg_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88,  50, 100, ..., 626,  43,  14])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7042707"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whole', 'close', 'they', 'killer', 'book', 'call', 'son', 'white', 'please', 'word', 'hardly', '10', 'novel', 'decent', 'talk', 'side']\n"
     ]
    }
   ],
   "source": [
    "valid_window = 1000  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "\n",
    "print( list(valid_examples) | select( lambda i: reverse_word_index[i-3] ) | as_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05514634, 0.05400386, 0.00447231, 0.05850446, 0.0075467 ,\n",
       "       0.00666854, 0.02504643, 0.02230966, 0.02370823, 0.02623644,\n",
       "       0.05946091, 0.06144324, 0.01182596, 0.01926584, 0.02713901,\n",
       "       0.02297904, 0.01097171, 0.0167491 , 0.02052677, 0.03402446,\n",
       "       0.05943317, 0.02657021, 0.01547801, 0.06284042, 0.05770507,\n",
       "       0.01490169, 0.01666608, 0.06153949, 0.03193345, 0.06212477,\n",
       "       0.02936892, 0.06156151, 0.02777913, 0.01954469, 0.06051388,\n",
       "       0.02978747, 0.01926054, 0.06079759, 0.01526949, 0.03187932,\n",
       "       0.05914519, 0.06070565, 0.0590456 , 0.01586687, 0.06007691,\n",
       "       0.06347959, 0.02991313, 0.06612297, 0.03813794, 0.05921679,\n",
       "       0.01594413, 0.02717398, 0.03928409, 0.06493407, 0.06029411,\n",
       "       0.05657085, 0.03994871, 0.05702174, 0.05965512, 0.06459205,\n",
       "       0.05866748, 0.05224859, 0.02990228, 0.05771722, 0.05565057,\n",
       "       0.02339513, 0.02571374, 0.05855092, 0.05900454, 0.05953146,\n",
       "       0.05767694, 0.02109058, 0.06303152, 0.0629799 , 0.06352007,\n",
       "       0.05883784, 0.0361346 , 0.03167587, 0.06007218, 0.05539999,\n",
       "       0.06144174, 0.06112008, 0.03351978, 0.06047047, 0.05959541,\n",
       "       0.05613201, 0.05878836, 0.03377074, 0.02412515, 0.0576916 ,\n",
       "       0.06309273, 0.06320039, 0.03844472, 0.06050001, 0.0588107 ,\n",
       "       0.06201539, 0.05694709, 0.06063254, 0.03450576, 0.05943348,\n",
       "       0.02375183, 0.06093617, 0.05843469, 0.03629017, 0.02536028,\n",
       "       0.06179453, 0.0285323 , 0.03223523, 0.05710334, 0.05797501,\n",
       "       0.06126695, 0.06178438, 0.016907  , 0.03328492, 0.05994939,\n",
       "       0.05947838, 0.05765354, 0.02516907, 0.0587066 , 0.06021801,\n",
       "       0.06100946, 0.05941519, 0.06094746, 0.06230243, 0.0304671 ,\n",
       "       0.05857494, 0.05972248, 0.06187678, 0.06210047, 0.0611697 ,\n",
       "       0.02653482, 0.06232426, 0.06536502, 0.05921888, 0.02901346,\n",
       "       0.03112923, 0.05746374, 0.0605974 , 0.05526742, 0.05655154,\n",
       "       0.05871081, 0.03631707, 0.05787928, 0.06018936, 0.02484426,\n",
       "       0.05732441, 0.05793935, 0.03437319, 0.06098568, 0.0630957 ,\n",
       "       0.02560529, 0.06068689, 0.07327176, 0.0601109 , 0.05776891,\n",
       "       0.0573486 , 0.05797419, 0.05525015, 0.05612799, 0.05855541,\n",
       "       0.05981357, 0.06284373, 0.06173817, 0.05505629, 0.05842354,\n",
       "       0.05898265, 0.05927191, 0.02782126, 0.06373534, 0.0560142 ,\n",
       "       0.05760847, 0.05792268, 0.01826402, 0.03410203, 0.06033756,\n",
       "       0.06148367, 0.06059659, 0.05573382, 0.03535557, 0.06046459,\n",
       "       0.05802607, 0.06521135, 0.05969858, 0.05797116, 0.06183671,\n",
       "       0.05799722, 0.06072113, 0.05603535, 0.05486904, 0.05844159,\n",
       "       0.06064859, 0.05503888, 0.02128439, 0.06524385, 0.02804133,\n",
       "       0.06302896, 0.05636689, 0.05748531, 0.05597277, 0.05931917,\n",
       "       0.06103658, 0.06089198, 0.05386057, 0.06140104, 0.05988493,\n",
       "       0.0585929 , 0.05443605, 0.06287892, 0.05503377, 0.06358471,\n",
       "       0.05428195, 0.05768577, 0.0634904 , 0.05950672, 0.06491224,\n",
       "       0.03341256, 0.06408215, 0.0596654 , 0.05955363, 0.05891122,\n",
       "       0.05735503, 0.06088105, 0.05813244, 0.05915433, 0.03177176,\n",
       "       0.05784297, 0.02119473, 0.05450179, 0.05610399, 0.05770065,\n",
       "       0.0548335 , 0.05984839, 0.06060338, 0.05924704, 0.05742739,\n",
       "       0.05668703, 0.06165406, 0.05744382, 0.06415073, 0.05708727,\n",
       "       0.06156299, 0.05801928, 0.06143894, 0.05705439, 0.0602729 ,\n",
       "       0.06496634, 0.05921837, 0.05798301, 0.06233947, 0.06136737,\n",
       "       0.05939636, 0.061894  , 0.0589558 , 0.05929247, 0.05768564,\n",
       "       0.05796588, 0.01973457, 0.05383516, 0.05854148, 0.06086591,\n",
       "       0.06083652, 0.05906283, 0.05833709, 0.05929777, 0.05735544,\n",
       "       0.06187771, 0.05942854, 0.06408897, 0.05591279, 0.06174127,\n",
       "       0.059949  , 0.05901629, 0.05815135, 0.05831834, 0.05759077,\n",
       "       0.06150103, 0.07062274, 0.06156895, 0.05759572, 0.05348134,\n",
       "       0.06180556, 0.06215388, 0.05749464, 0.02746215, 0.02375442,\n",
       "       0.06010398, 0.06278165, 0.05856925, 0.05938302, 0.05872903,\n",
       "       0.05652667, 0.05602792, 0.05857038, 0.06098638, 0.05908206,\n",
       "       0.05475271, 0.05792605, 0.03362001, 0.06180644, 0.05799586,\n",
       "       0.0595726 , 0.06231776, 0.06027553, 0.0591431 , 0.05741462,\n",
       "       0.06439561, 0.06434494, 0.06160628, 0.06182407, 0.06163794,\n",
       "       0.05964166, 0.06111127, 0.06254328, 0.06185698, 0.06921378,\n",
       "       0.06054074, 0.02383829, 0.02624432, 0.06472223, 0.059517  ,\n",
       "       0.05765685, 0.06264533, 0.05822262, 0.06036419, 0.05538389,\n",
       "       0.05823724, 0.05887509, 0.05549114, 0.05876937, 0.06368878,\n",
       "       0.05989058, 0.05634578, 0.05814067, 0.05765837, 0.05807334,\n",
       "       0.05532873, 0.02161811, 0.05648522, 0.05782587, 0.05611764,\n",
       "       0.05873716, 0.06223282, 0.06054452, 0.05857402, 0.0615664 ,\n",
       "       0.05978005, 0.06010276, 0.06310622, 0.06386005, 0.06374541,\n",
       "       0.06272292, 0.06018407, 0.06126799, 0.06135819, 0.0573892 ,\n",
       "       0.06317674, 0.06119337, 0.05906055, 0.06370801, 0.06328797,\n",
       "       0.05753228, 0.05862752, 0.06283287, 0.06551735, 0.0576828 ,\n",
       "       0.05798791, 0.06908254, 0.06044848, 0.06349484, 0.05972951,\n",
       "       0.0611011 , 0.060995  , 0.06288033, 0.05850799, 0.05864944,\n",
       "       0.06270386, 0.05695313, 0.0572629 , 0.05648522, 0.06008001,\n",
       "       0.05557633, 0.02917174, 0.06325975, 0.05842379, 0.06082311,\n",
       "       0.0208875 , 0.02354961, 0.06292055, 0.05646726, 0.05678468,\n",
       "       0.05670855, 0.06037913, 0.05554294, 0.05819416, 0.06553316,\n",
       "       0.05634214, 0.05850803, 0.06175083, 0.05812511, 0.05875112,\n",
       "       0.02101346, 0.05706783, 0.05926731, 0.0609362 , 0.05909872,\n",
       "       0.05820798, 0.0595611 , 0.02977902, 0.05955585, 0.06078313,\n",
       "       0.05771902, 0.05820954, 0.0585914 , 0.06134843, 0.06150092,\n",
       "       0.06017439, 0.05838804, 0.0595702 , 0.06123006, 0.06028589,\n",
       "       0.06285766, 0.05886112, 0.05970858, 0.06523447, 0.05581639,\n",
       "       0.05702282, 0.05736542, 0.05979348, 0.05811836, 0.06490919,\n",
       "       0.06193307, 0.06859975, 0.05758814, 0.06137127, 0.05798892,\n",
       "       0.05486986, 0.06213112, 0.05764024, 0.06013037, 0.05830505,\n",
       "       0.05601322, 0.0578147 , 0.06520948, 0.06113   , 0.05955825,\n",
       "       0.05925715, 0.06057148, 0.01705563, 0.05440977, 0.05970786,\n",
       "       0.05815425, 0.05864282, 0.05387507, 0.06509019, 0.05704841,\n",
       "       0.06139293, 0.0602225 , 0.05593715, 0.02838421, 0.0598481 ,\n",
       "       0.06402864, 0.05775998, 0.05937083, 0.05726767, 0.05758853,\n",
       "       0.06066282, 0.06212779, 0.05749199, 0.05913246, 0.02495541,\n",
       "       0.0552032 , 0.06177211, 0.05742829, 0.05535633, 0.05920325,\n",
       "       0.06310872, 0.01819403, 0.06225444, 0.05482974, 0.0611386 ,\n",
       "       0.01408477, 0.05858161, 0.06219414, 0.05904122, 0.05965997,\n",
       "       0.06387675, 0.06550383, 0.06089819, 0.05909329, 0.06011353,\n",
       "       0.05383879, 0.06113563, 0.06438787, 0.05875082, 0.05760796,\n",
       "       0.05759459, 0.06013914, 0.054616  , 0.05975088, 0.05695212,\n",
       "       0.05928775, 0.06068082, 0.06054109, 0.05987168, 0.05774102,\n",
       "       0.0610646 , 0.06204145, 0.05492328, 0.0583994 , 0.05877451,\n",
       "       0.05899495, 0.05429223, 0.06089212, 0.05598819, 0.06286678,\n",
       "       0.02334583, 0.0587275 , 0.06075474, 0.06250398, 0.06444284,\n",
       "       0.05707649, 0.0606856 , 0.06071825, 0.06081695, 0.0610591 ,\n",
       "       0.0564907 , 0.06436855, 0.05759945, 0.06313412, 0.05847976,\n",
       "       0.01934741, 0.06063335, 0.05858731, 0.06032388, 0.0612391 ,\n",
       "       0.06256172, 0.06309897, 0.05999652, 0.05756051, 0.05983938,\n",
       "       0.05820271, 0.0578497 , 0.06049987, 0.06036425, 0.06110345,\n",
       "       0.05725233, 0.02263521, 0.05897395, 0.05586091, 0.05761708,\n",
       "       0.05687951, 0.05820762, 0.06313507, 0.06057206, 0.05665671,\n",
       "       0.06206078, 0.05843263, 0.05784344, 0.06385794, 0.0597374 ,\n",
       "       0.05973853, 0.0611497 , 0.05814944, 0.05531896, 0.05593137,\n",
       "       0.06266969, 0.05790433, 0.0570832 , 0.05941483, 0.06070502,\n",
       "       0.10352342, 0.06187789, 0.05822422, 0.05734349, 0.05987847,\n",
       "       0.05429662, 0.05634164, 0.06106446, 0.05991194, 0.06330329,\n",
       "       0.05995687, 0.06014095, 0.06334413, 0.05924235, 0.05840683,\n",
       "       0.05894339, 0.06314541, 0.05887347, 0.06103859, 0.05728178,\n",
       "       0.05745349, 0.05769288, 0.05909102, 0.05825093, 0.05525697,\n",
       "       0.05961064, 0.06078844, 0.05704086, 0.06130808, 0.06043973,\n",
       "       0.05796348, 0.05848058, 0.06043795, 0.06501445, 0.05738692,\n",
       "       0.0624032 , 0.05997203, 0.05977903, 0.06275091, 0.0579662 ,\n",
       "       0.05992403, 0.05584263, 0.06114193, 0.05507652, 0.06163998,\n",
       "       0.06427155, 0.05755237, 0.05585526, 0.05791324, 0.02365479,\n",
       "       0.06267735, 0.06286635, 0.05845703, 0.05866806, 0.06005208,\n",
       "       0.05916411, 0.02380938, 0.06222562, 0.05914264, 0.05636935,\n",
       "       0.06208384, 0.0589429 , 0.06104518, 0.05913056, 0.05999194,\n",
       "       0.05751133, 0.06107238, 0.05683357, 0.061044  , 0.05938299,\n",
       "       0.06153237, 0.06022272, 0.0532871 , 0.06234216, 0.05650846,\n",
       "       0.06588541, 0.05808824, 0.05807576, 0.05742577, 0.05674944,\n",
       "       0.0614884 , 0.05841513, 0.05862543, 0.05688176, 0.06183814,\n",
       "       0.0589418 , 0.05331552, 0.05810787, 0.05830357, 0.05872809,\n",
       "       0.06130358, 0.06048933, 0.0593176 , 0.05917368, 0.05948908,\n",
       "       0.05972813, 0.06111254, 0.05784179, 0.06420971, 0.05531962,\n",
       "       0.01725897, 0.062641  , 0.05491385, 0.05965744, 0.05884806,\n",
       "       0.05734722, 0.06292562, 0.06009189, 0.06059786, 0.06026383,\n",
       "       0.06191393, 0.05976025, 0.05478245, 0.06208489, 0.06008905,\n",
       "       0.05925255, 0.06730258, 0.05409988, 0.06275372, 0.0621815 ,\n",
       "       0.06025057, 0.05885164, 0.06422354, 0.05826121, 0.05841853,\n",
       "       0.05959813, 0.05788358, 0.05846674, 0.06001643, 0.0633065 ,\n",
       "       0.05925137, 0.06156397, 0.05581002, 0.06198088, 0.06225203,\n",
       "       0.05858635, 0.05599111, 0.06271087, 0.05826281, 0.0620256 ,\n",
       "       0.06352314, 0.06207783, 0.05875032, 0.05997757, 0.05711249,\n",
       "       0.05768101, 0.05832977, 0.05904115, 0.05401633, 0.0559676 ,\n",
       "       0.05622969, 0.06378175, 0.05691845, 0.02281878, 0.05752803,\n",
       "       0.06069771, 0.06112045, 0.06342115, 0.0558098 , 0.06077946,\n",
       "       0.06608176, 0.05983442, 0.05771335, 0.05772173, 0.05711421,\n",
       "       0.05731236, 0.0616561 , 0.05616395, 0.05700953, 0.0568594 ,\n",
       "       0.05857279, 0.05846758, 0.06554485, 0.05958165, 0.058281  ,\n",
       "       0.05971673, 0.06306933, 0.05744367, 0.05802826, 0.06163043,\n",
       "       0.05663733, 0.05756225, 0.05777472, 0.06099108, 0.05983733,\n",
       "       0.05906331, 0.06018613, 0.05870229, 0.05932159, 0.0555311 ,\n",
       "       0.05664892, 0.06012735, 0.06309725, 0.06179276, 0.0623867 ,\n",
       "       0.0570309 , 0.05770364, 0.05759214, 0.05869361, 0.05484489,\n",
       "       0.05705659, 0.05647121, 0.05881095, 0.05550782, 0.06260543,\n",
       "       0.06243113, 0.06082462, 0.05870992, 0.06274298, 0.05895374,\n",
       "       0.05370564, 0.06237729, 0.05915653, 0.05868213, 0.05838444,\n",
       "       0.05318768, 0.05903558, 0.05911233, 0.05890228, 0.06007273,\n",
       "       0.05932191, 0.05855684, 0.0570728 , 0.05514201, 0.05887931,\n",
       "       0.05915015, 0.06391006, 0.06318459, 0.06175614, 0.06428198,\n",
       "       0.05804373, 0.05735357, 0.05628837, 0.06005476, 0.05718292,\n",
       "       0.05639857, 0.06181815, 0.05979911, 0.06281437, 0.0589744 ,\n",
       "       0.0568128 , 0.05608685, 0.05741594, 0.05948756, 0.06135489,\n",
       "       0.05968066, 0.05949855, 0.05640324, 0.06198698, 0.05935744,\n",
       "       0.06201665, 0.05634698, 0.05827741, 0.05661424, 0.06004299,\n",
       "       0.0659141 , 0.0590358 , 0.06128569, 0.0588413 , 0.06155573,\n",
       "       0.05984118, 0.06010347, 0.0619832 , 0.05876248, 0.05801316,\n",
       "       0.06009457, 0.06285266, 0.06054563, 0.01937152, 0.06204317,\n",
       "       0.05579156, 0.05862381, 0.05542818, 0.05705331, 0.05951967,\n",
       "       0.05968809, 0.05934758, 0.05908   , 0.05754681, 0.0608976 ,\n",
       "       0.06360632, 0.05942877, 0.0644634 , 0.06188903, 0.0584491 ,\n",
       "       0.05996561, 0.05828406, 0.0603979 , 0.05870682, 0.05947356,\n",
       "       0.05860358, 0.05984681, 0.06229206, 0.05798352, 0.06318776,\n",
       "       0.06315992, 0.06078104, 0.0625443 , 0.05482218, 0.058397  ,\n",
       "       0.06565599, 0.05973908, 0.0587782 , 0.0595702 , 0.06111591,\n",
       "       0.05986395, 0.06098737, 0.06151191, 0.06148086, 0.06006873,\n",
       "       0.05806908, 0.06218684, 0.0592525 , 0.06340998, 0.05755712,\n",
       "       0.06184405, 0.05670154, 0.06198879, 0.05889456, 0.05929926,\n",
       "       0.06182796, 0.05850072, 0.05809246, 0.0558176 , 0.05578333,\n",
       "       0.06214933, 0.05646595, 0.05617724, 0.05794053, 0.06092966,\n",
       "       0.05613033, 0.0586472 , 0.06001274, 0.05841221, 0.06257227,\n",
       "       0.05862891, 0.05844185, 0.0625575 , 0.05896446, 0.06220352,\n",
       "       0.06184002, 0.05764582, 0.06053537, 0.05811701, 0.05746091,\n",
       "       0.05605924, 0.06089942, 0.06181933, 0.06124448, 0.06389858,\n",
       "       0.05933528, 0.05684943, 0.05974162, 0.05722712, 0.05845604,\n",
       "       0.05758289, 0.05395914, 0.06242521, 0.0607682 , 0.05951345,\n",
       "       0.06164506, 0.05365509, 0.05916106, 0.05562852, 0.06077012,\n",
       "       0.0588668 , 0.06238816, 0.05440727, 0.0585909 , 0.06054872,\n",
       "       0.05452961, 0.0601815 , 0.05784733, 0.06142538, 0.05755452,\n",
       "       0.05664838, 0.05853503, 0.05924643, 0.06117793, 0.05774767,\n",
       "       0.05806325, 0.05807971, 0.05936971, 0.05627495, 0.06239875,\n",
       "       0.05830479, 0.05870487, 0.06125985, 0.06183191, 0.05774144,\n",
       "       0.05964307, 0.05764594, 0.05949952, 0.05613103, 0.06599215,\n",
       "       0.05751787, 0.06506611, 0.0627533 , 0.05851799, 0.05958992,\n",
       "       0.06321058, 0.06377359, 0.06017411, 0.02185038, 0.05796624,\n",
       "       0.06009322, 0.05544662, 0.06091719, 0.05784417, 0.06353603,\n",
       "       0.05467437, 0.0591735 , 0.06133048, 0.05721317, 0.06102879,\n",
       "       0.05522177, 0.05930143, 0.05468975, 0.06025428, 0.06392223,\n",
       "       0.06269483, 0.06192762, 0.06137911, 0.06720366, 0.05927661,\n",
       "       0.05804696, 0.06180963, 0.0583046 , 0.05854454, 0.05815516])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sim(570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553692,) (1094440,) (2553692,) (1094440,)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(word_target, word_context, test_size=0.3,random_state=42)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 200)\n",
      "input_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "maxlen=200\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 200, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "=================================================================\n",
      "Total params: 32,000\n",
      "Trainable params: 32,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected flatten_1 to have shape (6400,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-d98cdc9c50a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m history = model.fit(input_train, train_labels, batch_size=BATCH_SIZE,\n\u001b[1;32m---> 15\u001b[1;33m                     epochs=NUM_EPOCHS, verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected flatten_1 to have shape (6400,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "vocab_size = 1000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32, input_length=200))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(input_train, train_labels, batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-cd572920ea78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot loss function\n",
    "plt.subplot(211)\n",
    "plt.title(\"accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n",
    "\n",
    "# using the word2vec model\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# retrieve the weights from the first dense layer. This will convert\n",
    "# the input vector from a one-hot sum of two words to a dense 300\n",
    "# dimensional representation\n",
    "W, b = model.layers[0].get_weights()\n",
    "\n",
    "idx2emb = {}\n",
    "for word in word2idx.keys():\n",
    "    wid = word2idx[word]\n",
    "    vec_in = ohe.fit_transform(np.array(wid)).todense()\n",
    "    vec_emb = np.dot(vec_in, W)\n",
    "    idx2emb[wid] = vec_emb\n",
    "\n",
    "for word in [\"stupid\", \"alice\", \"succeeded\"]:\n",
    "    wid = word2idx[word]\n",
    "    source_emb = idx2emb[wid]\n",
    "    distances = []\n",
    "    for i in range(1, vocab_size):\n",
    "        if i == wid:\n",
    "            continue\n",
    "        target_emb = idx2emb[i]\n",
    "        distances.append(((wid, i),\n",
    "                         cosine_distances(source_emb, target_emb)))\n",
    "    sorted_distances = sorted(distances, key=operator.itemgetter(1))[0:10]\n",
    "    predictions = [idx2word[x[0][1]] for x in sorted_distances]\n",
    "    print(\"{:s} => {:s}\".format(word, \", \".join(predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
